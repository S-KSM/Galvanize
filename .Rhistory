}
classify_emotion <- function(textColumns,algorithm="bayes",prior=1.0,verbose=FALSE,...) {
matrix <- create_matrix(textColumns,...)
lexicon <- read.csv(system.file("data/emotions.csv.gz",package="sentiment"),header=FALSE)
counts <- list(anger=length(which(lexicon[,2]=="anger")),disgust=length(which(lexicon[,2]=="disgust")),fear=length(which(lexicon[,2]=="fear")),joy=length(which(lexicon[,2]=="joy")),sadness=length(which(lexicon[,2]=="sadness")),surprise=length(which(lexicon[,2]=="surprise")),total=nrow(lexicon))
documents <- c()
for (i in 1:nrow(matrix)) {
if (verbose) print(paste("DOCUMENT",i))
scores <- list(anger=0,disgust=0,fear=0,joy=0,sadness=0,surprise=0)
doc <- matrix[i,]
words <- findFreqTerms(doc,lowfreq=1)
for (word in words) {
for (key in names(scores)) {
emotions <- lexicon[which(lexicon[,2]==key),]
index <- pmatch(word,emotions[,1],nomatch=0)
if (index > 0) {
entry <- emotions[index,]
category <- as.character(entry[[2]])
count <- counts[[category]]
score <- 1.0
if (algorithm=="bayes") score <- abs(log(score*prior/count))
if (verbose) {
print(paste("WORD:",word,"CAT:",category,"SCORE:",score))
}
scores[[category]] <- scores[[category]]+score
}
}
}
if (algorithm=="bayes") {
for (key in names(scores)) {
count <- counts[[key]]
total <- counts[["total"]]
score <- abs(log(count/total))
scores[[key]] <- scores[[key]]+score
}
} else {
for (key in names(scores)) {
scores[[key]] <- scores[[key]]+0.000001
}
}
best_fit <- names(scores)[which.max(unlist(scores))]
if (best_fit == "disgust" && as.numeric(unlist(scores[2]))-3.09234 < .01) best_fit <- NA
documents <- rbind(documents,c(scores$anger,scores$disgust,scores$fear,scores$joy,scores$sadness,scores$surprise,best_fit))
}
colnames(documents) <- c("ANGER","DISGUST","FEAR","JOY","SADNESS","SURPRISE","BEST_FIT")
return(documents)
}
##### Required Packages in R
library(plyr)
library(ggplot2)
library(twitteR)
library(ggplot2)
library(httpuv)
library(RTextTools)
library(RColorBrewer)
library(RCurl)
library(e1071)
library(rjson)
library(tm)
library(wordcloud)
library(maxent)
#####################  Setting up Authorization to login to twitter ###########################
consumer_key='etLgBacjlil8lp5g9lRddgbXe'
consumer_secret='y2jbZD5uqBjp2s7EYX7UYZU7MzfXw1X6Hdj2uGrgHQjEoF5oGG'
access_token='1687602692-5kkt7K7AIEIq31eIRDW5lPQPPyAhGkS4g6MKrqR'
access_secret='pdKprgD1KQVT8rqXjFK0yjtNZJdOQF120eqUUhAulrcxl'
setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_secret)
######################## Posting the Query Ã¬# Drone RegulationÃ® ###########################
Drone=searchTwitteR("#Drone",lang="en",n=1000)
some_txt = sapply(Drone, function(x) x$getText())
# remove retweet entities
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
# remove at people
some_txt = gsub("@\\w+", "", some_txt)
# remove punctuation
some_txt = gsub("[[:punct:]]", "", some_txt)
# remove numbers
some_txt = gsub("[[:digit:]]", "", some_txt)
# remove html links
some_txt = gsub("http\\w+", "", some_txt)
# remove unnecessary spaces
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
# define "tolower error handling" function
try.error = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
# lower case using try.error with sapply
some_txt = sapply(some_txt, try.error)
# remove NAs in some_txt
some_txt = some_txt[!is.na(some_txt)]
names(some_txt) = NULL
# Perform Sentiment Analysis
# classify emotion
class_emo = classify_emotion(some_txt, algorithm="bayes", prior=1.0)
# get emotion best fit
emotion = class_emo[,7]
# substitute NA's by "unknown"
emotion[is.na(emotion)] = "unknown"
# classify polarity
class_pol = classify_polarity(some_txt, algorithm="bayes")
# get polarity best fit
polarity = class_pol[,4]
# Create data frame with the results and obtain some general statistics
# data frame with results
sent_df = data.frame(text=some_txt, emotion=emotion,
polarity=polarity, stringsAsFactors=FALSE)
# sort data frame
sent_df = within(sent_df,
emotion <- factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE)))
)
##### The fina l data with the extracted tweets is labelled sent_df. ########
######## Code for Algorithm ########
#### generate class using #######
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
# we got a vector of sentences. plyr will handle a list or a vector as an "l" for us
# we want a simple array of scores back, so we use "l" + "a" + "ply" = laply:
scores = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
positives= readLines("positive-words.txt")
negatives = readLines("negative-words.txt")
score=score.sentiment(sent_df$text,positives,negatives)
####### Convert document to term ############
matrix= create_matrix(sent_df$text, language="english",  removeStopwords=TRUE, removeNumbers=TRUE,  stemWords=TRUE, stripWhitespace=TRUE,minDocFreq=2)
features=as.matrix(matrix)
drone=data.frame(response=(score$score),features)
drone_=drone[-c(which(drone$response==0)),]
res=ifelse(drone_$response>0,1,-1)
dronedat=data.frame(res,drone_[,-1])
####### create train:test partition ############
index=sample(1:nrow(dronedat),round(0.80*nrow(dronedat)))
train=dronedat[index,]
test=dronedat[-index,]
################## Fit SVM ######################################################
svmm=svm(res~.,data=train,kernel="linear",scale=FALSE)
pred=predict(svmm,test[,-1])
predy=ifelse(pred>0,1,-1)
tab=table(test[,1],predy)
accuracy_svm=sum(diag(tab))/(sum(tab))
accuracy_svm
summary(svmm)
############################### Fit Naive Bayes #########################################
nbbayes=naiveBayes(train[,-1],train[,1],data=train)
fit1=predict(nbbayes,test[,-1])
tab1=table(test[,1],fit1)
accuracy_nb=sum(diag(tab1))/(sum(tab1))
accuracy_nb
summary(nbbayes)
########################################## Fit MaxEnt ############################
maxentt=maxent(train[,-1],train[,1])
fit2=predict(maxentt,newdata=test[,-1])
tab2=table(test[,1],fit2)
accuracy_maxent=sum(diag(tab2))/(sum(tab2))
accuracy_maxent
summary(maxentt)
######################################## Classifier Accuracy ##########################
accuracy_svm
summary(svmm)
accuracy_nb
summary(nbbayes)
accuracy_maxent
summary(maxentt)
View(sent_df)
CMAT=rbind(accuracy_nb,accuracy_maxent,accuracy_svm)
CMAT
classify_emotion <- function(textColumns,algorithm="bayes",prior=1.0,verbose=FALSE,...) {
matrix <- create_matrix(textColumns,...)
lexicon <- read.csv("emotions.csv",header = FALSE)
counts <- list(anger=length(which(lexicon[,2]=="anger")),disgust=length(which(lexicon[,2]=="disgust")),fear=length(which(lexicon[,2]=="fear")),joy=length(which(lexicon[,2]=="joy")),sadness=length(which(lexicon[,2]=="sadness")),surprise=length(which(lexicon[,2]=="surprise")),total=nrow(lexicon))
documents <- c()
for (i in 1:nrow(matrix)) {
if (verbose) print(paste("DOCUMENT",i))
scores <- list(anger=0,disgust=0,fear=0,joy=0,sadness=0,surprise=0)
doc <- matrix[i,]
words <- findFreqTerms(doc,lowfreq=1)
for (word in words) {
for (key in names(scores)) {
emotions <- lexicon[which(lexicon[,2]==key),]
index <- pmatch(word,emotions[,1],nomatch=0)
if (index > 0) {
entry <- emotions[index,]
category <- as.character(entry[[2]])
count <- counts[[category]]
score <- 1.0
if (algorithm=="bayes") score <- abs(log(score*prior/count))
if (verbose) {
print(paste("WORD:",word,"CAT:",category,"SCORE:",score))
}
scores[[category]] <- scores[[category]]+score
}
}
}
if (algorithm=="bayes") {
for (key in names(scores)) {
count <- counts[[key]]
total <- counts[["total"]]
score <- abs(log(count/total))
scores[[key]] <- scores[[key]]+score
}
} else {
for (key in names(scores)) {
scores[[key]] <- scores[[key]]+0.000001
}
}
best_fit <- names(scores)[which.max(unlist(scores))]
if (best_fit == "disgust" && as.numeric(unlist(scores[2]))-3.09234 < .01) best_fit <- NA
documents <- rbind(documents,c(scores$anger,scores$disgust,scores$fear,scores$joy,scores$sadness,scores$surprise,best_fit))
}
colnames(documents) <- c("ANGER","DISGUST","FEAR","JOY","SADNESS","SURPRISE","BEST_FIT")
return(documents)
}
########################################################
classify_polarity <- function(textColumns,algorithm="bayes",pstrong=0.5,pweak=1.0,prior=1.0,verbose=FALSE,...) {
matrix <- create_matrix(textColumns,...)
lexicon <- read.csv("subjectivity.csv",header=FALSE)
counts <- list(positive=length(which(lexicon[,3]=="positive")),negative=length(which(lexicon[,3]=="negative")),total=nrow(lexicon))
documents <- c()
for (i in 1:nrow(matrix)) {
if (verbose) print(paste("DOCUMENT",i))
scores <- list(positive=0,negative=0)
doc <- matrix[i,]
words <- findFreqTerms(doc,lowfreq=1)
for (word in words) {
index <- pmatch(word,lexicon[,1],nomatch=0)
if (index > 0) {
entry <- lexicon[index,]
polarity <- as.character(entry[[2]])
category <- as.character(entry[[3]])
count <- counts[[category]]
score <- pweak
if (polarity == "strongsubj") score <- pstrong
if (algorithm=="bayes") score <- abs(log(score*prior/count))
if (verbose) {
print(paste("WORD:",word,"CAT:",category,"POL:",polarity,"SCORE:",score))
}
scores[[category]] <- scores[[category]]+score
}
}
if (algorithm=="bayes") {
for (key in names(scores)) {
count <- counts[[key]]
total <- counts[["total"]]
score <- abs(log(count/total))
scores[[key]] <- scores[[key]]+score
}
} else {
for (key in names(scores)) {
scores[[key]] <- scores[[key]]+0.000001
}
}
best_fit <- names(scores)[which.max(unlist(scores))]
ratio <- as.integer(abs(scores$positive/scores$negative))
if (ratio==1) best_fit <- "neutral"
documents <- rbind(documents,c(scores$positive,scores$negative,abs(scores$positive/scores$negative),best_fit))
if (verbose) {
print(paste("POS:",scores$positive,"NEG:",scores$negative,"RATIO:",abs(scores$positive/scores$negative)))
cat("\n")
}
}
colnames(documents) <- c("POS","NEG","POS/NEG","BEST_FIT")
return(documents)
}
########################################################
create_matrix <- function(textColumns, language="english", minDocFreq=1, minWordLength=3, removeNumbers=TRUE, removePunctuation=TRUE, removeSparseTerms=0, removeStopwords=TRUE, stemWords=FALSE, stripWhitespace=TRUE, toLower=TRUE, weighting=weightTf) {
stem_words <- function(x) {
split <- strsplit(x," ")
return(wordStem(split[[1]],language=language))
}
control <- list(language=language,tolower=toLower,removeNumbers=removeNumbers,removePunctuation=removePunctuation,stripWhitespace=stripWhitespace,minWordLength=minWordLength,stopwords=removeStopwords,minDocFreq=minDocFreq,weighting=weighting)
if (stemWords == TRUE) control <- append(control,list(stemming=stem_words),after=6)
trainingColumn <- apply(as.matrix(textColumns),1,paste,collapse=" ")
trainingColumn <- sapply(as.vector(trainingColumn,mode="character"),iconv,to="UTF8",sub="byte")
corpus <- Corpus(VectorSource(trainingColumn),readerControl=list(language=language))
matrix <- DocumentTermMatrix(corpus,control=control);
if (removeSparseTerms > 0) matrix <- removeSparseTerms(matrix,removeSparseTerms)
gc()
return(matrix)
}
library(plyr)
library(ggplot2)
library(twitteR)
library(ggplot2)
library(httpuv)
library(RTextTools)
library(RColorBrewer)
library(RCurl)
library(e1071)
library(rjson)
library(tm)
library(wordcloud)
library(maxent)
install.packages("twitteR")
install.packages("RTextTools")
naiveBayes()
?naiveBayes
data(HouseVotes84, package = "mlbench")
model <- naiveBayes(Class ~ ., data = HouseVotes84)
predict(model, HouseVotes84[1:10,])
predict(model, HouseVotes84[1:10,], type = "raw")
pred <- predict(model, HouseVotes84)
table(pred, HouseVotes84$Class)
HouseVotes84
data(HouseVotes84, package = "mlbench")
model <- naiveBayes(Class ~ ., data = HouseVotes84)
predict(model, HouseVotes84[1:10,])
predict(model, HouseVotes84[1:10,], type = "raw")
pred <- predict(model, HouseVotes84)
table(pred, HouseVotes84$Class)
names(HouseVotes84)
train
library(dplyr) ## lead()
setwd("C:/Users/Shobeir/Desktop/Work/Fiverr/20160417/William/")
hisfunc <- function(){
data <- read.csv("audcad_input.csv")
## create additiona 4 columns: Mid,Tick,Micro Price and    Micro Tick
data$Mid <- (data$Bid_price+data$Ask_price)/2
data$Tick <- ifelse(abs(log(data$Mid/lead(data$Mid,1)))>0.0001,data$Mid,'')
data$Micro_Price <- ((data$Bid_price*data$Ask_volume)+(data$Ask_price*data$Bid_volume))/(data$Bid_volume+data$Ask_volume)
data$Micro_Tick <- ifelse(abs(log(data$Micro_Price/lead(data$Micro_Price,1)))>0.0001,data$Micro_Price,'')
tmp <- data[grep('0',data$Micro_Tick)+1,]
## set variables
event <- 'up'
P_t0 <- tmp[1,"Micro_Price"]
P_low <- P_t0
P_high <- P_t0
t0_dc <- NULL
t1_dc <- NULL
t0_os <- NULL
t1_os <- NULL
threshold <-  0.0010
Timestamp <- NULL
Micro_Price_Tick <- NULL
Extreme_Price <- NULL
Directional_Change_Up <- NULL
Directional_Change_Down <- NULL
Directional_Change_Confirmation_Point <- NULL
OverShoot_Starts_Price <- NULL
Overshoot_Ends_at_Extreme <- NULL
Total_Move_Magnitude_Log_Return <- NULL
prev_event <- NULL
Prev_Extreme_Price <- NULL
for (i in 1:nrow(tmp)){
P_now <- tmp[i,"Micro_Price"]
if (event=='up'){
if(P_now <= P_high * (1-threshold)){
prev_event <- 'up'
event <- 'down'
P_low <- P_now
} else {
if(P_high < P_now){
P_high <- P_now
}
}
} else {
if(P_now >= P_low * (1+threshold)){
prev_event <- 'down'
event <- 'up'
P_high <- P_now
} else {
if(P_low > P_now){
P_low <- P_now
}
}
}
#cat(paste(paste0('Processing event #',i,'/#',nrow(tmp))),'\n')
## set variables to output files
Timestamp[i] <- as.character(tmp[i,1][1])
Micro_Price_Tick[i] <- P_now[1]
Extreme_Price[i] <- ifelse(event=='up',P_low,P_high)
Directional_Change_Up[i] <- ifelse(is.null(prev_event),0,ifelse(event=='up' & prev_event=='down',1,0))
Directional_Change_Down[i] <- ifelse(is.null(prev_event),0,ifelse(event=='down' & prev_event=='up',1,0))
Directional_Change_Confirmation_Point[i] <- ifelse(Directional_Change_Up[i]==1 | Directional_Change_Down[i]==1,P_now,'')
OverShoot_Starts_Price[i] <- ifelse(Directional_Change_Up[i]==1 | Directional_Change_Down[i]==1,P_now,'')
Overshoot_Ends_at_Extreme[i] <- ifelse(!is.null(prev_event) && event!=prev_event,Prev_Extreme_Price,'')
Total_Move_Magnitude_Log_Return[i] <- ifelse(!is.null(prev_event) && Directional_Change_Up[i]==1 | Directional_Change_Down[i]==1,ifelse(Extreme_Price[i]==Prev_Extreme_Price,'',log(Extreme_Price[i]/Prev_Extreme_Price)),'')
prev_event <- event
Prev_Extreme_Price <- Extreme_Price[i]
if(OverShoot_Starts_Price[i]!=''){
if(length(grep('0',Overshoot_Ends_at_Extreme))>1){
Overshoot_Ends_at_Extreme[i-1] <- Overshoot_Ends_at_Extreme[i]
Overshoot_Ends_at_Extreme[i] <- ''
}
}
}
## create data.frame
out <- data.frame(Timestamp,Micro_Price_Tick,Extreme_Price,Directional_Change_Confirmation_Point,Directional_Change_Up,Directional_Change_Down,OverShoot_Starts_Price,Overshoot_Ends_at_Extreme,Total_Move_Magnitude_Log_Return)
## write to file
write.table(out,'expected_results.csv',quote=F,row.names=F,append=F,col.names=T,sep=",")
}
tmp
hisfunc <- function(){
data <- read.csv("audcad_input.csv")
## create additiona 4 columns: Mid,Tick,Micro Price and    Micro Tick
data$Mid <- (data$Bid_price+data$Ask_price)/2
data$Tick <- ifelse(abs(log(data$Mid/lead(data$Mid,1)))>0.0001,data$Mid,'')
data$Micro_Price <- ((data$Bid_price*data$Ask_volume)+(data$Ask_price*data$Bid_volume))/(data$Bid_volume+data$Ask_volume)
data$Micro_Tick <- ifelse(abs(log(data$Micro_Price/lead(data$Micro_Price,1)))>0.0001,data$Micro_Price,'')
tmp <- data[grep('0',data$Micro_Tick)+1,]
print(tmp)
print()
print()
print()
## set variables
event <- 'up'
P_t0 <- tmp[1,"Micro_Price"]
P_low <- P_t0
P_high <- P_t0
t0_dc <- NULL
t1_dc <- NULL
t0_os <- NULL
t1_os <- NULL
threshold <-  0.0010
Timestamp <- NULL
Micro_Price_Tick <- NULL
Extreme_Price <- NULL
Directional_Change_Up <- NULL
Directional_Change_Down <- NULL
Directional_Change_Confirmation_Point <- NULL
OverShoot_Starts_Price <- NULL
Overshoot_Ends_at_Extreme <- NULL
data <- read.csv("audcad_input.csv")
library(dplyr) ## lead()
#hisfunc <- function(){
data <- read.csv("audcad_input.csv")
data <- read.csv("./audcad_input.csv")
data <- read.csv("C:/Users/Shobeir/Desktop/Work/Fiverr/20160503/williamn/audcad_input.csv")
## create additiona 4 columns: Mid,Tick,Micro Price and    Micro Tick
data$Mid <- (data$Bid_price+data$Ask_price)/2
data$Tick <- ifelse(abs(log(data$Mid/lead(data$Mid,1)))>0.0001,data$Mid,'')
data$Micro_Price <- ((data$Bid_price*data$Ask_volume)+(data$Ask_price*data$Bid_volume))/(data$Bid_volume+data$Ask_volume)
data$Micro_Tick <- ifelse(abs(log(data$Micro_Price/lead(data$Micro_Price,1)))>0.0001,data$Micro_Price,'')
tmp <- data[grep('0',data$Micro_Tick)+1,]
print(tmp)
head(tmp)
?lead
lead(1:10, 1)
dim(data)
dim(tmp)
tmp$Micro_Price
tmp$Micro_Tick
# Classification Tree with rpart
library(rpart)
# grow tree
fit <- rpart(Kyphosis ~ Age + Number + Start,
method="class", data=kyphosis)
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
# plot tree
plot(fit, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
# create attractive postscript plot of tree
post(fit, file = "c:/tree.ps",
title = "Classification Tree for Kyphosis")
post(fit, file = "C:/tree.ps",
title = "Classification Tree for Kyphosis")
post(fit, file = "C://tree.ps",
title = "Classification Tree for Kyphosis")
mydata = read.csv2(file = "C:/Users/Shobeir/Downloads/dataNavid.csv",sep = ",")
dim(mydata)
setwd("C:/Users/Shobeir/Desktop/")
mytbl <- read.csv("./tests.csv")
mytbl
chisq.test(mytbl)
chisq.test(mytbl)
str(mytbl)
summary(mytbl)
chisq.test(tbl(mytbl))
chisq.test(table(mytbl))
chisq.test(table(mytbl)[,2:3])
mytbl
mytbl[1:2]
mytbl[2:3]
chisq.test(mytbl[2:3])
chisq.test(mytbl[2:3])$expected
